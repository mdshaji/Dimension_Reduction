# 3 Cluster Solution
fit <- kmeans(normalized_data, 3)
str(fit)
fit$cluster
final <- data.frame(fit$cluster, DATA) # Append cluster membership
wine <- aggregate(DATA[, 1:14], by = list(fit$cluster), FUN = mean)
wine <- aggregate(DATA[, 1:13], by = list(fit$cluster), FUN = mean)
library(readr)
write_csv(wine, "Wine_kmeans_R.csv")
getwd()
# Training Data - Data file is imported by Text(base) to convert strings into factors
Salary_train <- read.csv(file.choose())
View(Salary_train)
str(Salary_train)
attach(Salary_train)
Salary_train$educationno <- as.factor(Salary_train$educationno)
class(Salary_train)
# Test Data - Data file is imported by Text(base) to convert strings into factors
Salary_test <- read.csv(file.choose())
View(Salary_test)
str(Salary_test)
Salary_test$educationno <- as.factor(Salary_test$educationno)
class(Salary_test)
summary(Salary_train)
summary(Salary_test)
plot(workclass,Salary, main = "Workclass")
plot(workclass,Salary, main = "Workclass")
Salary_train <- read.csv("D:/Module 19 - Naive Bayes/SalaryData_Train.csv", stringsAsFactors=TRUE)
View(Salary_train)
Salary_test <- read.csv("D:/Module 19 - Naive Bayes/SalaryData_Test.csv", stringsAsFactors=TRUE)
View(Salary_test)
View(Salary_train)
str(Salary_train)
attach(Salary_train)
Salary_train$educationno <- as.factor(Salary_train$educationno)
class(Salary_train)
View(Salary_test)
str(Salary_test)
Salary_test$educationno <- as.factor(Salary_test$educationno)
class(Salary_test)
summary(Salary_train)
summary(Salary_test)
plot(workclass,Salary, main = "Workclass")
plot(education,Salary, main = "Education")
plot(occupation,Salary, main = "Occupation")
plot(relationship,Salary, main = "Relationship")
library(ggplot2)
ggplot(data= Salary_train,aes(x=Salary, y = age, fill = Salary)) +
geom_boxplot() + ggtitle("Box Plot")
ggplot(data=Salary_train,aes(x=Salary, y = hoursperweek, fill = Salary)) +
geom_boxplot() +
ggtitle("Box Plot")
ggplot(data=Salary_train,aes(x = age, fill = Salary)) +
geom_density(alpha = 0.9, color = 'Violet')
ggplot(data=Salary_train,aes(x = workclass, fill = Salary)) +
geom_density(alpha = 0.9, color = 'Violet')
ggplot(data=Salary_train,aes(x = education, fill = Salary)) +
geom_density(alpha = 0.9, color = 'Violet')
ggplot(data=Salary_train,aes(x = educationno, fill = Salary)) +
geom_density(alpha = 0.9, color = 'Violet')
# proportion of salary
prop.table(table(Salary_test$Salary))
prop.table(table(Salary_train$Salary))
##  Training a model on the data ----
install.packages("e1071")
library(e1071)
# Naive Bayes Model
Model <- naiveBayes(Salary_train$Salary ~ ., data = Salary_train)
Model
##  Evaluating model performance
Salary_test_pred <- predict(Model, Salary_test)
CrossTable(Salary_test_pred, Salary_test$Salary,
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
library(gmodels)
CrossTable(Salary_test_pred, Salary_test$Salary,
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
test_acc = mean(Salary_test_pred == Salary_test$Salary)
test_acc
# On Training Data
Salary_train_pred <- predict(Model, Salary_train)
train_acc = mean(Salary_train_pred == Salary_train$Salary)
train_acc
#Installing and loading the libraries
#install.packages("recommenderlab", dependencies=TRUE)
#install.packages("Matrix")
library("recommenderlab")
library(caTools)
#movie rating data
Joke_rate_data <- read.csv(file.choose())
#movie rating data
Joke_rate_data <- read.xlsx(file.choose())
#movie rating data
Joke_rate_data <- read.excel(file.choose())
#movie rating data
library(readxl)
Joke_rate_data <- read.excel(file.choose())
Joke_rate_data <- read_xlsx(file.choose())
#metadata about the variable
str(Joke_rate_data)
#rating distribution
hist(Joke_rate_data$rating)
View(Joke_rate_data)
View(Joke_rate_data)
Joke_rate_data <- read_xlsx(file.choose())
Joke_rate_data <- Joke_rate_data[ ,2:4]
#metadata about the variable
str(Joke_rate_data)
#rating distribution
hist(Joke_rate_data$rating)
View(Joke_rate_data)
#rating distribution
hist(Joke_rate_data$Rating)
#the datatype should be realRatingMatrix inorder to build recommendation engine
movie_rate_data_matrix <- as(movie_rate_data, 'realRatingMatrix')
#the datatype should be realRatingMatrix inorder to build recommendation engine
Joke_rate_data_matrix <- as(Joke_rate_data, 'realRatingMatrix')
is.na.sum()
?is.na
library("recommenderlab")
library(caTools)
library(reshape2)
# Jokes Rating Data
library(readxl)
Joke_rate_data <- read_xlsx(file.choose())
View(Joke_rate_data)
head(Joke_rate_data)
# Removing unnecessary column of "Id"
Joke_rate_data <- Joke_rate_data[ ,2:4]
dim(Joke_rate_data)
is.na(Joke_rate_data)
sum(is.na(Joke_rate_data))
# Convert to Matrix format
ratings_matrix <- as.matrix(acast(Joke_rate_data , user_id ~ joke_id, fun.aggregate = mean))
dim(ratings_matrix)
# Recommender lab real rating matrix format
R <- as(ratings_matrix, "realRatingMatrix")
rec1 <- Recommender(R, method="UBCF") # User-based collaberative filtering
rec2 <- Recommender(R, method="IBCF") # Item Based collaberative filtering
rec3 <- Recommender(R, method="SVD")
rec4 <- Recommender(R, method="POPULAR")
rec5 <- Recommender(binarize(R,minRating = 2), method="UBCF")
# Creating recommendations for users
uid <- 333
Jokes <- subset(Joke_rate_data, Joke_rate_data$user_id==uid)
View(Jokes)
print("You have rated")
print("You have rated:")
Jokes
print("recommendations for you:")
Jokes
prediction <- predict(rec1, R[uid], n=2)
View(prediction)
prediction <- predict(rec1, R[uid], n=2)
as(prediction, "list")
prediction <- predict(rec2, R[uid], n=2)
as(prediction, "list")
prediction <- predict(rec3, R[uid], n=2)
as(prediction, "list")
prediction <- predict(rec4, R[uid], n=2)
as(prediction, "list")
prediction <- predict(rec5, R[uid], n=2)
as(prediction, "list")
# Creating recommendations for users
uid <- 4
Jokes <- subset(Joke_rate_data, Joke_rate_data$user_id==uid)
print("You have rated:")
Jokes
print("recommendations for you:")
prediction <- predict(rec1, R[uid], n=2) ## we can change the model here
as(prediction, "list")
prediction <- predict(rec2, R[uid], n=2)
as(prediction, "list")
prediction <- predict(rec3, R[uid], n=2)
as(prediction, "list")
prediction <- predict(rec4, R[uid], n=2)
as(prediction, "list")
prediction <- predict(rec5, R[uid], n=2)
as(prediction, "list")
# Creating recommendations for user_id # 26002
uid <- 26002
Jokes <- subset(Joke_rate_data, Joke_rate_data$user_id==uid)
print("You have rated:")
Jokes
print("recommendations for you:")
prediction <- predict(rec1, R[uid], n=2) ## we can change the model here
as(prediction, "list")
prediction <- predict(rec2, R[uid], n=2)
# Creating recommendations for user_id # 500
uid <- 500
Jokes <- subset(Joke_rate_data, Joke_rate_data$user_id==uid)
print("You have rated:")
Jokes
print("recommendations for you:")
prediction <- predict(rec1, R[uid], n=2) ## we can change the model here
as(prediction, "list")
prediction <- predict(rec2, R[uid], n=2)
as(prediction, "list")
prediction <- predict(rec3, R[uid], n=2)
as(prediction, "list")
prediction <- predict(rec4, R[uid], n=2)
as(prediction, "list")
prediction <- predict(rec5, R[uid], n=2)
as(prediction, "list")
input <- read.csv(file.choose())
View(input)
## Removing unncessary columns
data <- input[, -1]
attach(data)
summary(data)
boxplot(Alcohol, col = "orange",main = "Alcohol")
boxplot(Ash, col = "purple",main = "Ash")
boxplot(Alcalinity, col = "red",main = "Alcalinity")
boxplot(Magnesium, col = "dodgerblue4",main = "Magnesium")
boxplot(Color, col = "pink", horizontal = T,main = "Color")
hist(Alcohol,col = "orange", main = "Alcohol" )
hist(Ash,col = "purple", main = "Ash")
hist(Alcalinity,col = "red", main = "Alcalinity")
hist(Magnesium,col = "blue", main = "Magnesium")
hist(Color,col = "pink", main = "Color")
pcaObj <- princomp(data, cor = TRUE, scores = TRUE, covmat = NULL)
str(pcaObj)
summary(pcaObj)
loadings(pcaObj)
plot(pcaObj) # graph showing importance of principal components
biplot(pcaObj)
plot(cumsum(pcaObj$sdev * pcaObj$sdev) * 100 / (sum(pcaObj$sdev * pcaObj$sdev)), type = "b")
pcaObj$scores
pcaObj$scores[, 1:3]
# Top 3 pca scores
final <- cbind(input[, 1], pcaObj$scores[, 1:3])
View(final)
# Scatter diagram
plot(final)
input <- read.csv(file.choose())
View(input)
## Removing unnecessary columns
Data <- input[, -1]
summary(Data)
# Normalize the data
normalized_data <- scale(Data[, 1:13])
summary(normalized_data)
# Distance matrix
d <- dist(normalized_data, method = "euclidean")
fit <- hclust(d, method = "ward.D2")
# Display dendrogram
plot(fit)
plot(fit, hang = -1)
groups <- cutree(fit, k =14)# Cut tree into 14 clusters
rect.hclust(fit, k =14, border = "red")
cluster <- as.matrix(groups)
final <- data.frame(cluster, Data)
aggregate(Data[, 1:11], by = list(final$cluster), FUN = mean)
library(readr)
input <- read.csv(file.choose())
View(input)
## Removing unnecessary columns
DATA <- input[, -1]
attach(DATA)
str(DATA)
summary(DATA)
# Normalize the data
normalized_data <- scale(DATA[, 1:13]) # As we already removed "Type" column so all columns need to normalize
summary(normalized_data)
# Elbow curve to decide the k value
twss <- NULL
for (i in 2:13) {
twss <- c(twss, kmeans(normalized_data, centers = i)$tot.withinss)
}
twss
# Look for an "elbow" in the scree plot
plot(2:13, twss, type = "b", xlab = "Number of Clusters", ylab = "Within groups sum of squares")
# 3 Cluster Solution
fit <- kmeans(normalized_data, 3)
str(fit)
fit$cluster
final <- data.frame(fit$cluster, DATA) # Append cluster membership
wine <- aggregate(DATA[, 1:13], by = list(fit$cluster), FUN = mean)
reticulate::repl_python()
import pandas as pd
no
quit
Salary_train <- read.csv("D:/Module 19 - Naive Bayes/SalaryData_Train.csv", stringsAsFactors=TRUE)
View(Salary_train)
Salary_test <- read.csv("D:/Module 19 - Naive Bayes/SalaryData_Test.csv", stringsAsFactors=TRUE)
View(Salary_test)
str(Salary_train)
attach(Salary_train)
Salary_train$educationno <- as.factor(Salary_train$educationno)
class(Salary_train)
# Test Data - Data file is imported by Text(base) to convert strings into factors
Salary_test <- read.csv(file.choose())
str(Salary_test)
Salary_test$educationno <- as.factor(Salary_test$educationno)
class(Salary_test)
summary(Salary_train)
summary(Salary_test)
plot(workclass,Salary, main = "Workclass")
plot(education,Salary, main = "Education")
plot(occupation,Salary, main = "Occupation")
plot(relationship,Salary, main = "Relationship")
library(ggplot2)
ggplot(data= Salary_train,aes(x=Salary, y = age, fill = Salary)) +
geom_boxplot() + ggtitle("Box Plot")
ggplot(data=Salary_train,aes(x=Salary, y = hoursperweek, fill = Salary)) +
geom_boxplot() +
ggtitle("Box Plot")
ggplot(data=Salary_train,aes(x = age, fill = Salary)) +
geom_density(alpha = 0.9, color = 'Violet')
ggplot(data=Salary_train,aes(x = workclass, fill = Salary)) +
geom_density(alpha = 0.9, color = 'Violet')
ggplot(data=Salary_train,aes(x = education, fill = Salary)) +
geom_density(alpha = 0.9, color = 'Violet')
ggplot(data=Salary_train,aes(x = educationno, fill = Salary)) +
geom_density(alpha = 0.9, color = 'Violet')
# proportion of salary
prop.table(table(Salary_test$Salary))
prop.table(table(Salary_train$Salary))
##  Training a model on the data ----
install.packages("e1071")
library(e1071)
# Naive Bayes Model
Model <- naiveBayes(Salary_train$Salary ~ ., data = Salary_train)
Model
##  Evaluating model performance
Salary_test_pred <- predict(Model, Salary_test)
library(gmodels)
CrossTable(Salary_test_pred, Salary_test$Salary,
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
test_acc = mean(Salary_test_pred == Salary_test$Salary)
test_acc
# On Training Data
Salary_train_pred <- predict(Model, Salary_train)
train_acc = mean(Salary_train_pred == Salary_train$Salary)
train_acc
# Import the raw_sms dataset
library(readr)
sms_raw <- read_csv("D:/Module 19 - Naive Bayes/sms_raw_NB.csv")
View(sms_raw)
str(sms_raw)
sms_raw$type <- factor(sms_raw$type)
# examine the type variable more carefully
str(sms_raw$type)
table(sms_raw$type)
# proportion of ham and spam messages
prop.table(table(sms_raw$type))
library(tm)
str(sms_raw$text)
sms_corpus <- Corpus(VectorSource(sms_raw$text))
sms_corpus <- tm_map(sms_corpus, function(x) iconv(enc2utf8(x), sub='byte'))
# clean up the corpus using tm_map()
corpus_clean <- tm_map(sms_corpus, tolower)
corpus_clean <- tm_map(corpus_clean, removePunctuation)
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
# create a document-term sparse matrix
sms_dtm <- DocumentTermMatrix(corpus_clean)
sms_dtm
View(sms_dtm[1:10, 1:30])
# To view DTM we need to convert it into matrix first
dtm_matrix <- as.matrix(sms_dtm)
str(dtm_matrix)
View(dtm_matrix[1:10, 1:20])
colnames(sms_dtm)[1:50]
# creating training and test datasets
sms_raw_train <- sms_raw[1:4169, ]
sms_raw_test  <- sms_raw[4170:5559, ]
sms_corpus_train <- corpus_clean[1:4169]
sms_corpus_test  <- corpus_clean[4170:5559]
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test  <- sms_dtm[4170:5559, ]
# check that the proportion of spam is similar
prop.table(table(sms_raw$type))
prop.table(table(sms_raw_train$type))
prop.table(table(sms_raw_test$type))
# indicator features for frequent words
# dictionary of words which are used more than 5 times
sms_dict <- findFreqTerms(sms_dtm_train, 5)
sms_train <- DocumentTermMatrix(sms_corpus_train, list(dictionary = sms_dict))
sms_test  <- DocumentTermMatrix(sms_corpus_test, list(dictionary = sms_dict))
sms_test_matrix <- as.matrix(sms_test)
View(sms_test_matrix[1:10,1:10])
convert_counts <- function(x) {
x <- ifelse(x > 0, 1, 0)
x <- factor(x, levels = c(0, 1), labels = c("No", "Yes"))
}
# apply() convert_counts() to columns of train/test data
# Margin = 2 is for columns
# Margin = 1 is for rows
sms_train <- apply(sms_train, MARGIN = 2, convert_counts)
sms_test  <- apply(sms_test, MARGIN = 2, convert_counts)
View(sms_test[1:10,1:10])
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_raw_train$type)
sms_classifier
##  Evaluating model performance
sms_test_pred <- predict(sms_classifier, sms_test)
CrossTable(sms_test_pred, sms_raw_test$type,
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
# Load the dataset
library(readxl)
Airline <- read_excel(file.choose())
View(Airline)
# Removing unnecessary columns
Data <- Airline[2:12]
View(Data)
str(Data)
summary(Data)
# Normalize the data
normalized_data <- scale(Data[, 1:11]) # As we already removed ID column so all columns need to normalize
summary(normalized_data)
# Elbow curve to decide the k value
twss <- NULL
for (i in 2:12) {
twss <- c(twss, kmeans(normalized_data, centers = i)$tot.withinss)
}
twss
# Look for an "elbow" in the scree plot
plot(2:12, twss, type = "b", xlab = "Number of Clusters", ylab = "Within groups sum of squares")
title(sub = "K-Means Clustering Scree-Plot")
# 3 Cluster Solution
fit <- kmeans(normalized_data, 3)
str(fit)
fit$cluster
final <- data.frame(fit$cluster, Data) # Append cluster membership
aggregate(Data[, 1:11], by = list(fit$cluster), FUN = mean)
# Load the dataset
library(readxl)
Airline <- read_excel(file.choose())
View(Airline)
# Removing unnecessary columns
Data <- Airline[2:12]
View(Data)
str(Data)
summary(Data)
attach(Data)
boxplot(Balance, col = "orange",main = "Balance")
boxplot(Bonus_miles, col = "yellow",main = "Bonus Miles")
boxplot(Flight_miles_12mo, col = "dodgerblue4",main = "Flight Miles")
boxplot(Bonus_trans, col = "red", horizontal = T,main = "Bonus Trans")
boxplot(Days_since_enroll, col = "dodgerblue4",main = "Days Since Enroll")
boxplot(Bonus_trans, col = "red", horizontal = T,main = "Bonus Trans")
hist(Balance, col = "orange",main = "Balance")
hist(Bonus_miles, col = "yellow",main = "Bonus Miles")
hist(Days_since_enroll, col = "blue",main = "Days Since Enroll")
hist(Bonus_trans,col = "red", main = "Bonus Trans")
boxplot(Bonus_miles, col = "yellow",main = "Bonus Miles")
# Load the dataset
crime <- read.csv(file.choose())
View(crime)
summary(crime)
attach(crime)
boxplot(Murder, col = "dodgerblue4",main = "Murder")
boxplot(Assault, col = "dodgerblue4",main = "Assault")
boxplot(UrbanPop, col = "dodgerblue4",main = "UrbanPop")
boxplot(Rape, col = "red", horizontal = T,main = "Rape")
hist(Murder,col = "orange", main = "Murder" )
hist(Assault,col = "yellow", main = "Assault")
hist(UrbanPop,col = "blue", main = "UrbanPop")
hist(Rape,col = "red", main = "Rape")
# Normalize the data
normalized_data <- scale(crime[, 2:5])
summary(normalized_data)
install.packages("plyr")
library(plyr)
# Elbow curve to decide the k value
twss <- NULL
for (i in 2:8) {
twss <- c(twss, kmeans(normalized_data, centers = i)$tot.withinss)
}
twss
plot(2:8, twss, type = "b", xlab = "Number of Clusters", ylab = "Within groups sum of squares")
title(sub = "K-Means Clustering Scree-Plot")
# 3 Cluster Solution
fit <- kmeans(normalized_data, 3)
str(fit)
fit$cluster
final <- data.frame(fit$cluster, crime) # Append cluster membership
aggregate(crime[, 2:5], by = list(fit$cluster), FUN = mean)
# Load the dataset
Insurance <- read.csv(file.choose())
View(Insurance)
summary(Insurance)
attach(Insurance)
boxplot(Premiums.Paid, col = "dodgerblue4",main = "Premium Paid")
boxplot(Claims.made, col = "dodgerblue4",main = "Claims")
boxplot(Income, col = "red", horizontal = T,main = "Income")
hist(Premiums.Paid,col = "orange", main = "Premium Paid" )
hist(Claims.made,col = "blue", main = "Claims")
hist(Income,col = "red", main = "Income")
# Normalize the data
normalized_data <- scale(Insurance[, 1:5])
summary(normalized_data)
library(plyr)
# Elbow curve to decide the k value
twss <- NULL
for (i in 2:8) {
twss <- c(twss, kmeans(normalized_data, centers = i)$tot.withinss)
}
twss
plot(2:8, twss, type = "b", xlab = "Number of Clusters", ylab = "Within groups sum of squares")
title(sub = "K-Means Clustering Scree-Plot")
# 3 Cluster Solution
fit <- kmeans(normalized_data, 3)
str(fit)
fit$cluster
final <- data.frame(fit$cluster, Insurance) # Append cluster membership
aggregate(Insurance[, 2:5], by = list(fit$cluster), FUN = mean)
